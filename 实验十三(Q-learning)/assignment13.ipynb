{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f093a6",
   "metadata": {},
   "source": [
    "**<font color = black size=6>实验十三: Q-learning</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e79026",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第一部分: Q-learning 算法介绍</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067480d6",
   "metadata": {},
   "source": [
    "**1) Q-learning 伪代码**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10517a",
   "metadata": {},
   "source": [
    "![Q-learning.jpg](Q-learning.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f401c",
   "metadata": {},
   "source": [
    "初始化 Q-table  \n",
    "每个 epoch 循环:  \n",
    "&emsp;&emsp;初始化 state $S$  \n",
    "&emsp;&emsp;每一个 step 循环:  \n",
    "&emsp;&emsp;&emsp;&emsp;根据贪婪程度选择一个 action $A$  \n",
    "&emsp;&emsp;&emsp;&emsp;开始进行 action $A$，观察得到的 reward $R$, 下一个 state $S^{'}$  \n",
    "&emsp;&emsp;&emsp;&emsp;更新 Q-table 中的 Q($S$,$A$): Q($S$,$A$) = Q($S$,$A$) + $\\alpha [R + \\gamma \\max_{a^{'}}Q(S^{'},a^{'}) - Q(S,A)]$  \n",
    "&emsp;&emsp;&emsp;&emsp;将当前状态 $S$ 更新为 $S^{'}$  \n",
    "&emsp;&emsp;直到$S$到达终点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709df4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c953911",
   "metadata": {},
   "source": [
    "**2)迷宫**  \n",
    "假设在某个 5x5 大小的迷宫中,玩家从起点(0,0)出发，中间包括四种地段：  \n",
    "1.路径(正常通过，每次经过会获得-1 的奖励，游戏继续)；  \n",
    "2.障碍(到达后游戏结束，到达会获得-5 的奖励)；  \n",
    "3.陷阱(到达后游戏结束，到达会获得-15 的奖励)；  \n",
    "4.终点(到达后游戏结束，到达会获得+25 的奖励)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623eb599",
   "metadata": {},
   "source": [
    "我们用下列矩阵代表迷宫，S(0,0)代表起点，E(4,4)代表终点，T(1,1)代表陷阱,B(3,1)和(1,3)代表障碍，其余 0 代表路径  \n",
    "[S,0,0,0,0],  \n",
    " [0,T,0,B,0],  \n",
    " [0,0,0,0,0],  \n",
    " [0,B,0,0,0],  \n",
    " [0,0,0,0,E],\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446821d",
   "metadata": {},
   "source": [
    "Action: 每次玩家有四种行动方式（上，下，右，左），分别用 0，1，2，3 代表\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cf859",
   "metadata": {},
   "source": [
    "Q-table: Q-learning 中最重要的即为其拥有的 Q-table，代表着该算法中的行为准则。每次进行行动前都需要根据 Q-table 来进行判断，每次算法学习即为更新 Q-table。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31259139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [0, 1, 2, 3]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 定义一个空的Q-table\n",
    "action_space = [\"up\", \"down\", \"left\", \"right\"]\n",
    "actions = len(action_space)\n",
    "q_table = pd.DataFrame(columns=list(range(actions)), dtype=np.float64)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ea429",
   "metadata": {},
   "source": [
    "$\\epsilon$-greedy: 因为在初始阶段, 随机的探索环境, 往往比固定的行为模式要好, 所以这也是累积经验的阶段, 我们希望探索者不会那么贪婪(greedy)。 所以 $\\epsilon$ 就是用来控制贪婪程度的值。$\\epsilon$ = 0.9 意味着 90% 的时间是选择最优策略, 10% 的时间来探索。  \n",
    "$\\gamma$ 是对未来 reward 的衰减值  \n",
    "$\\alpha$是学习率, 来决定这次的误差有多少是要被学习的, $\\alpha$是一个小于 1 的数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d562947",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # 自己设置\n",
    "alpha = 0.9  # 自己设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd796fd",
   "metadata": {},
   "source": [
    "环境反馈：当玩家从某个 state $s$通过某个 action $a$到达另一个 state $s\\_$后，会获得一定的奖励$r$。而后我们就会根据 Q-learning 算法使用$(s,a,s\\_,r)$更新 Q-table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a3000",
   "metadata": {},
   "source": [
    "例子：玩家在起点(0,0)，此时的 Q-table 如下所示：  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action  \n",
    "state&nbsp; &nbsp; &nbsp; 0 &nbsp; 1 &nbsp; 2 &nbsp; 3  \n",
    "[0,0]&nbsp; &nbsp; &nbsp; 0 &nbsp; -1 &nbsp; 0 &nbsp; 0  \n",
    "[0,1]&nbsp; &nbsp; &nbsp; 0 &nbsp; -1 &nbsp; 3 &nbsp; 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f56e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1    2    3\n",
      "[0, 0]  0.0 -1.0  0.0  0.0\n",
      "[0, 1]  0.0 -1.0  3.0  2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\662793476.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  q_table = q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\662793476.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  q_table = q_table.append(\n"
     ]
    }
   ],
   "source": [
    "q_table = q_table.append(\n",
    "    pd.Series(\n",
    "        [0, -1, 0, 0],\n",
    "        index=q_table.columns,\n",
    "        name=str([0, 0]),\n",
    "    )\n",
    ")\n",
    "q_table = q_table.append(\n",
    "    pd.Series(\n",
    "        [0, -1, 3, 2],\n",
    "        index=q_table.columns,\n",
    "        name=str([0, 1]),\n",
    "    )\n",
    ")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca01b7",
   "metadata": {},
   "source": [
    "此时玩家的 state 为(0,0)  \n",
    "他现在有两种行动方式，一种是向右(action = 2)，一种是向下(action = 1)。此时我们的贪婪程度为 0.9，所以我们随机生成一个 0 到 1 的浮点数。如果小于 0.9 则采用贪婪模式，即选择该 state 所对应的 action 中对应的 value 最大的一个 action；如果大于 0.9，则为探索模式，则我们从该 state 中的所有 action 中随机选取一个 action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f56a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [0, 0]\n",
    "# 生成随机数a\n",
    "# if a<epsilon:\n",
    "# 寻找该state s中value最大的action作为其action\n",
    "# else:\n",
    "# 该state s中所有action中随机选一个作为其action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f7ff7",
   "metadata": {},
   "source": [
    "这里我们采用贪婪模式，查询 Q-table 后发现 state 存在，其对应的四个 action 中，只有 action = 1 时的 value 最小，所以我们从剩下三个对应 value 最大的 action（0，2，3）中随机挑选一个 action，这里我们选择 action=2 作为 action  \n",
    "$s$ = (0,0)，action $a$ = 2 后，玩家到达下一个 state $s\\_$ = (0,1)为路径，所以收获的 reward 为$r$ = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d25bae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "s = str(s)\n",
    "s_ = str([0, 1])\n",
    "r = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23fbc3",
   "metadata": {},
   "source": [
    "之后该$(s,a,s\\_,r)$被用来更新 Q-table：\n",
    "$$Q(s,a) = Q(s,a) + \\alpha  (r + \\gamma  \\max_{a^{'}}Q(s\\_ ,a^{'}) - Q(s,a) )$$\n",
    "此时查询 Q-table，$Q(s,a)=0$, $\\alpha$和$\\gamma$都设为 0.9，查询$s\\_$对应的 action 中最大的 value 值，发现$\\max_{a^{'}} Q(s\\_ ,a^{'}) = 3$,所以更新后的 Q(s,a) = 0 + 0.9 _ (-1 + 0.9 _ 3 - 0) = 0.9 \\* 1.7 = 1.53\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2d13a",
   "metadata": {},
   "source": [
    "更新后的 Q-table：  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action  \n",
    "state&nbsp; &nbsp; &nbsp; 0 &nbsp; 1 &nbsp; 2 &nbsp; 3  \n",
    "[0,0]&nbsp; &nbsp; &nbsp; 0 &nbsp; -1 1.53 0  \n",
    "[0,1]&nbsp; &nbsp; &nbsp; 0 &nbsp; -1 &nbsp; 3 &nbsp; 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3a5c0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1     2    3\n",
      "[0, 0]  0.0 -1.0  1.53  0.0\n",
      "[0, 1]  0.0 -1.0  3.00  2.0\n"
     ]
    }
   ],
   "source": [
    "q_table.loc[s, a] += alpha * (\n",
    "    (r + gamma * q_table.loc[s_, :].max()) - q_table.loc[s, a]\n",
    ")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ad421",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第二部分: tkinter 库介绍</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7504e06",
   "metadata": {},
   "source": [
    "Tkinter 是 Python 的标准 GUI 库。我们可以利用该库实现 python 图形界面的建立\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b671adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2cb65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_unit = 80  # 像素\n",
    "maze_height = 4  # 迷宫高度\n",
    "maze_weight = 4  # 迷宫宽度\n",
    "\n",
    "\n",
    "class Grid_Paint(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Grid_Paint, self).__init__()\n",
    "        self.title(\"Grid_Paint\")\n",
    "        # 设置窗口的宽和高\n",
    "        self.geometry(\n",
    "            \"{0}x{1}\".format(maze_height * pixel_unit, maze_weight * pixel_unit)\n",
    "        )\n",
    "        # 调用构建迷宫函数来搭建迷宫\n",
    "        self._build_maze()\n",
    "\n",
    "    # 创造黑色的正方形方块\n",
    "    # 输入分别为【一个网格的相对中心位置】，【障碍的横坐标】，【障碍的纵坐标】\n",
    "    def creat_barrier(self, origin, x, y):\n",
    "        # 计算出方块的中心位置\n",
    "        center = origin + np.array([pixel_unit * x, pixel_unit * y])\n",
    "        # 以该中心位置向四周进行黑色填充生成黑色方块\n",
    "        self.barrier = self.canvas.create_rectangle(\n",
    "            center[0] - 25, center[1] - 25, center[0] + 25, center[1] + 25, fill=\"black\"\n",
    "        )\n",
    "        return self.barrier\n",
    "\n",
    "    # 构建迷宫\n",
    "    def _build_maze(self):\n",
    "        # 画出白色背景\n",
    "        self.canvas = tk.Canvas(\n",
    "            self,\n",
    "            bg=\"white\",\n",
    "            height=maze_height * pixel_unit,\n",
    "            width=maze_weight * pixel_unit,\n",
    "        )\n",
    "\n",
    "        # 通过画线来构建网格\n",
    "        for c in range(0, maze_weight * pixel_unit, pixel_unit):\n",
    "            x0, y0, x1, y1 = c, 0, c, maze_height * pixel_unit\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, maze_height * pixel_unit, pixel_unit):\n",
    "            x0, y0, x1, y1 = 0, r, maze_weight * pixel_unit, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # 每个网格的相对中心位置\n",
    "        origin = np.array([int(0.5 * pixel_unit), int(0.5 * pixel_unit)])\n",
    "\n",
    "        # （0，1）处创造黑色方块\n",
    "        self.barrier1 = self.creat_barrier(origin, 0, 1)\n",
    "\n",
    "        # （1，0）处创造红色圆形\n",
    "        red_center = origin + np.array([pixel_unit * 1, pixel_unit * 0])\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            red_center[0] - 25,\n",
    "            red_center[1] - 25,\n",
    "            red_center[0] + 25,\n",
    "            red_center[1] + 25,\n",
    "            fill=\"red\",\n",
    "        )\n",
    "\n",
    "        # （0，0）处创造绿色正方形\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 25, origin[1] - 25, origin[0] + 25, origin[1] + 25, fill=\"green\"\n",
    "        )\n",
    "\n",
    "        # （1，1）处创造粉色方块\n",
    "        pink_center = origin + np.array([pixel_unit * 1, pixel_unit * 1])\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            pink_center[0] - 25,\n",
    "            pink_center[1] - 25,\n",
    "            pink_center[0] + 25,\n",
    "            pink_center[1] + 25,\n",
    "            fill=\"pink\",\n",
    "        )\n",
    "\n",
    "        # 打包所有元素\n",
    "        self.canvas.pack()\n",
    "\n",
    "\n",
    "# grid = Grid_Paint()\n",
    "# grid.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe0e5d",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第三部分: 实验任务</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d9edf",
   "metadata": {},
   "source": [
    "按照上面介绍的 Grid_Paint 类进行修改（也可以自己重新设计一个类）,同时实现一个 QLearning 类来完成具体方法，要求完成的任务要求包括:  \n",
    "1.生成一个 6x6 的迷宫，其中包括起点，终点，障碍(至少 6 个)，陷阱(至少 1 个)；如果有其它设置也可以加入  \n",
    "2.使用 Q-learning 算法在若干轮迭代（例如 100 轮）后找到一条从起点到终点的最短路径，得分规则自行设计，规则合理即可  \n",
    "3.每轮迭代过程中需要在图形界面中显示玩家的移动过程，每次失败后更新 Q-table 表并重置玩家位置使得玩家能够继续在下一轮游戏中进行游戏  \n",
    "4.最后输出玩家成功通关的次数和失败的次数，以及最后的 Q-table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076ff49",
   "metadata": {},
   "source": [
    "下面为例子，帮助大家进行逻辑理解，仅作为参考，大家可以按照自己的思路完成，不强制要求按照下列逻辑完成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d56c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_unit = 80  # 像素\n",
    "maze_height = 6  # 迷宫高度\n",
    "maze_weight = 6  # 迷宫宽度\n",
    "\n",
    "\n",
    "class Grid_Paint(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Grid_Paint, self).__init__()\n",
    "        # 初始化动作指令\n",
    "        self.action_space = [\"u\", \"d\", \"l\", \"r\"]\n",
    "        # 方便之后用0，1，2，3代表上下左右\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.title(\"Grid_Paint\")\n",
    "        # 设置窗口的宽和高\n",
    "        self.geometry(\n",
    "            \"{0}x{1}\".format(maze_height * pixel_unit, maze_weight * pixel_unit)\n",
    "        )\n",
    "        # 调用构建迷宫函数来搭建迷宫\n",
    "        self._build_maze()\n",
    "\n",
    "    # 创造黑色的正方形方块\n",
    "    # 输入分别为【一个网格的相对中心位置】，【障碍的横坐标】，【障碍的纵坐标】\n",
    "    def creat_barrier(self, origin, x, y):\n",
    "        # 计算出方块的中心位置\n",
    "        center = origin + np.array([pixel_unit * x, pixel_unit * y])\n",
    "        # 以该中心位置向四周进行黑色填充生成黑色方块\n",
    "        self.barrier = self.canvas.create_rectangle(\n",
    "            center[0] - 25, center[1] - 25, center[0] + 25, center[1] + 25, fill=\"black\"\n",
    "        )\n",
    "        return self.barrier\n",
    "\n",
    "    # 构建迷宫\n",
    "    def _build_maze(self):\n",
    "        # 画出白色背景\n",
    "        self.canvas = tk.Canvas(\n",
    "            self,\n",
    "            bg=\"white\",\n",
    "            height=maze_height * pixel_unit,\n",
    "            width=maze_weight * pixel_unit,\n",
    "        )\n",
    "\n",
    "        # 通过画线来构建网格\n",
    "        for c in range(0, maze_weight * pixel_unit, pixel_unit):\n",
    "            x0, y0, x1, y1 = c, 0, c, maze_height * pixel_unit\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, maze_height * pixel_unit, pixel_unit):\n",
    "            x0, y0, x1, y1 = 0, r, maze_weight * pixel_unit, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # 每个网格的相对中心位置\n",
    "        origin = np.array([int(0.5 * pixel_unit), int(0.5 * pixel_unit)])\n",
    "\n",
    "        # 创造障碍  黑色方块\n",
    "        self.barrier1 = self.creat_barrier(origin, 0, 5)\n",
    "        self.barrier2 = self.creat_barrier(origin, 1, 1)\n",
    "        self.barrier3 = self.creat_barrier(origin, 3, 4)\n",
    "        self.barrier4 = self.creat_barrier(origin, 1, 5)\n",
    "        self.barrier5 = self.creat_barrier(origin, 2, 3)\n",
    "        self.barrier6 = self.creat_barrier(origin, 1, 2)\n",
    "\n",
    "        # 创造陷阱 （2,2）处\n",
    "        # 本人选择绿色方块作为陷阱\n",
    "        trap_center = origin + np.array([pixel_unit * 2, pixel_unit * 2])\n",
    "        self.trap = self.canvas.create_rectangle(\n",
    "            trap_center[0] - 25,\n",
    "            trap_center[1] - 25,\n",
    "            trap_center[0] + 25,\n",
    "            trap_center[1] + 25,\n",
    "            fill=\"green\",\n",
    "        )\n",
    "\n",
    "        # 迷宫中的实时位置\n",
    "        # 本人选择红色圆作为实时位置\n",
    "        red_center = origin + np.array([pixel_unit * 0, pixel_unit * 0])\n",
    "        self.now = self.canvas.create_oval(\n",
    "            red_center[0] - 25,\n",
    "            red_center[1] - 25,\n",
    "            red_center[0] + 25,\n",
    "            red_center[1] + 25,\n",
    "            fill=\"red\",\n",
    "        )\n",
    "\n",
    "        # 创造终点\n",
    "        # 本人选择蓝色方块\n",
    "        pink_center = origin + np.array([pixel_unit * 5, pixel_unit * 5])\n",
    "        self.end = self.canvas.create_rectangle(\n",
    "            pink_center[0] - 25,\n",
    "            pink_center[1] - 25,\n",
    "            pink_center[0] + 25,\n",
    "            pink_center[1] + 25,\n",
    "            fill=\"blue\",\n",
    "        )\n",
    "\n",
    "        # 打包所有元素\n",
    "        self.canvas.pack()\n",
    "\n",
    "    # 重置玩家位置，使得每一次游戏结束后玩家回到初始位置便于下一轮游戏，即回到起始位置\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.5)\n",
    "        self.canvas.delete(self.now)\n",
    "        # 红色圆形\n",
    "        origin = np.array([int(0.5 * pixel_unit), int(0.5 * pixel_unit)])\n",
    "        self.now = self.canvas.create_oval(\n",
    "            origin[0] - 25,\n",
    "            origin[1] - 25,\n",
    "            origin[0] + 25,\n",
    "            origin[1] + 25,\n",
    "            fill=\"red\",\n",
    "        )\n",
    "        # 返回初始位置\n",
    "        return self.canvas.coords(self.now)\n",
    "\n",
    "    # 玩家移动，输入为移动指令\n",
    "    def step(self, action):\n",
    "        # 记录当前的state，也就是玩家现在的位置，s是一个1x4的数组，分别代表其左上角像素位置，右上角像素位置，左下角像素位置，右下角像素位置\n",
    "        s = self.canvas.coords(self.now)\n",
    "        base_action = np.array([0, 0])\n",
    "\n",
    "        # 向上\n",
    "        if action == 0:\n",
    "            if s[1] > pixel_unit:\n",
    "                base_action[1] -= pixel_unit\n",
    "        # 向下\n",
    "        elif action == 1:\n",
    "            if s[1] < (maze_height - 1) * pixel_unit:\n",
    "                base_action[1] += pixel_unit\n",
    "        # 向右\n",
    "        elif action == 2:\n",
    "            if s[0] < (maze_weight - 1) * pixel_unit:\n",
    "                base_action[0] += pixel_unit\n",
    "        # 向左\n",
    "        elif action == 3:\n",
    "            if s[0] > pixel_unit:\n",
    "                base_action[0] -= pixel_unit\n",
    "        # 第一个参数是移动目标，第二个参数是到左上角的水平距离，第三个参数是距左上角的垂直距离。\n",
    "        self.canvas.move(self.now, base_action[0], base_action[1])\n",
    "\n",
    "        # 移动后的位置，也就是下一个state\n",
    "        s_ = self.canvas.coords(self.now)\n",
    "\n",
    "        # 将障碍物进行聚合\n",
    "        barriers = [\n",
    "            self.barrier1,\n",
    "            self.barrier2,\n",
    "            self.barrier3,\n",
    "            self.barrier4,\n",
    "            self.barrier5,\n",
    "            self.barrier6,\n",
    "        ]\n",
    "\n",
    "        # reward判断\n",
    "        # 如果碰到了陷阱，游戏结束\n",
    "        if s_ == self.canvas.coords(self.trap):\n",
    "            reward = -15\n",
    "            status = True\n",
    "            s_ = \"died\"\n",
    "\n",
    "        # 如果碰到了障碍，游戏结束\n",
    "        elif any(s_ == self.canvas.coords(barrier) for barrier in barriers):\n",
    "            reward = -5\n",
    "            status = True\n",
    "            s_ = \"died\"\n",
    "\n",
    "        # 如果到达了终点，则奖励为50，且游戏结束\n",
    "        elif s_ == self.canvas.coords(self.end):\n",
    "            reward = 50\n",
    "            status = True\n",
    "            s_ = \"terminal\"\n",
    "\n",
    "        # 如果都没有碰到，则游戏继续，但是奖励为-1，代表移动的步数，否则无法去寻找最低步数\n",
    "        else:\n",
    "            reward = -1\n",
    "            status = False\n",
    "\n",
    "        # 返回state s在经过action之后的下一个state s_，获得的奖励 reward，以及此时游戏状态 status\n",
    "        return s_, reward, status\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.1)\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11dcfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    # 初始化，包括action列表，学习率，衰弱率，贪婪程度，以及Q-table\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.9):\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    # 选择action，输入参数为当前的state，输出的为在当前state的下一步action\n",
    "    def choose_action(self, s):\n",
    "        self.check_state_exist(s)\n",
    "        # 首先判断该state在Q-table中是否存在，如果不存在则加入到Q-table\n",
    "        # action 选择\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # 贪婪模式\n",
    "            # 挑选最佳的action\n",
    "            state_action = self.q_table.loc[s, :]\n",
    "            # 如果有多个action的value值都是最大，那就从这些中随机挑选\n",
    "            a = np.random.choice(\n",
    "                state_action[state_action == np.max(state_action)].index\n",
    "            )\n",
    "        else:\n",
    "            # 非贪婪，探索模式\n",
    "            # 随机挑选action\n",
    "            a = np.random.choice(self.actions)\n",
    "        return a\n",
    "\n",
    "    # 学习以此不断更新Q-table，输入参数为一个state，做出的动作a，收获的奖励r，下一个state s_\n",
    "    def learn(self, s, a, r, s_):\n",
    "        # 首先判断下一个state s_在Q-table中是否存在，如果不存在则加入到Q-table\n",
    "        self.check_state_exist(s_)\n",
    "\n",
    "        # 先从Q-table中查询到Q(s,a)\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "\n",
    "        # 如果下一个state代表游戏结束，则不需要找下一个state s_能获得得最大value值，\n",
    "\n",
    "        # 如果下一个state游戏继续，则首先找到下一个state s_能获得的最大value值\n",
    "        if s_ != \"terminal\":\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "        else:\n",
    "            q_target = r\n",
    "        # 更新 Q-table\n",
    "        self.q_table.loc[s, a] += self.alpha * (q_target - q_predict)\n",
    "\n",
    "    # 检查state是否存在，输入为要检查的state\n",
    "    def check_state_exist(self, s):\n",
    "        if s not in self.q_table.index:\n",
    "            # 将新状态添加到 Q-table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0] * len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=s,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# 获取坐标\n",
    "def set_state(s):\n",
    "    loc = []\n",
    "    loc.append(int((s[0] - maze_weight) / pixel_unit))\n",
    "    loc.append(int((s[1] - maze_height) / pixel_unit))\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "242af273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_26368\\3980519262.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "正确次数: 100\n",
      "错误次数: 0\n",
      "                 0          1          2         3\n",
      "[0, 0]   -3.366884  -3.319892  -3.333053 -3.363819\n",
      "[1, 0]   -2.822695  -2.847664  -2.825867 -2.860123\n",
      "[0, 1]   -2.913115  -2.873703  -2.847664 -2.894468\n",
      "died      0.000000   0.000000   0.000000  0.000000\n",
      "[2, 0]   -2.296823  -2.257152  -2.234564 -2.350546\n",
      "[2, 1]   -1.769087  -2.850000  -1.749189 -2.047550\n",
      "[0, 2]   -2.385032  -2.402620  -2.608516 -2.437979\n",
      "[0, 3]   -1.919758  -1.956208  -1.954567 -1.970773\n",
      "[3, 0]   -1.654862  -1.652852  -1.699633 -1.783956\n",
      "[3, 1]   -1.312895  -1.253266  -0.454994 -1.309075\n",
      "[4, 1]   -0.917469  -0.647522   4.139016 -0.962645\n",
      "[4, 2]   -0.476453  -0.450100   3.477521 -0.480420\n",
      "[3, 2]   -0.699628  -0.729093  -0.215283 -1.500000\n",
      "[3, 3]   -0.347746  -0.950000  -0.190178 -0.500000\n",
      "[4, 3]   -0.216739  -0.271000   4.124538 -0.289712\n",
      "[0, 4]   -1.670450  -1.719500  -1.635543 -1.653792\n",
      "[4, 0]   -1.133043  -0.569455  -1.144356 -1.228075\n",
      "[5, 2]   -0.199810  26.308361   1.401143 -0.199000\n",
      "[5, 0]   -0.945419  -0.463809  -0.864828 -0.841248\n",
      "[1, 3]   -1.719500  -1.594245  -2.047550 -1.628172\n",
      "[5, 1]   -0.519654  13.275886  -0.585199 -0.621823\n",
      "[5, 3]   -0.109000  38.874761  -0.100000 -0.199000\n",
      "[5, 4]    3.862082  48.609358   4.184842  0.180732\n",
      "[4, 4]   -0.117910   1.571795   4.119240 -0.500000\n",
      "terminal  0.000000   0.000000   0.000000  0.000000\n",
      "[1, 4]   -1.384573  -1.719500  -1.380855 -1.344501\n",
      "[2, 4]   -0.950000  -0.919973  -0.950000 -0.943619\n",
      "[2, 5]   -0.418156  -0.490100  -0.014503 -1.355000\n",
      "[4, 5]   -0.199000  -0.100000  23.427950 -0.100000\n",
      "[3, 5]   -0.500000  -0.100000   4.378121 -0.320630\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 应用上面实现的两个类\n",
    "grid = Grid_Paint()\n",
    "RL = QLearning(actions=list(range(grid.n_actions)))\n",
    "EPOCH = 100\n",
    "right = 0\n",
    "wrong = 0\n",
    "for episode in range(EPOCH):\n",
    "    print(episode)\n",
    "    # 初始化玩家位置\n",
    "    s = grid.reset()\n",
    "    s = set_state(s)\n",
    "    # 开始走迷宫\n",
    "    while True:\n",
    "        # 基于当前状态s确定下一步的action\n",
    "        action = RL.choose_action(str(s))\n",
    "\n",
    "        # 当前state在采取行动a后的下一个state s_,获得的奖励 r，当前游戏状态status\n",
    "        s_, reward, status = grid.step(action)\n",
    "        if s_ != \"terminal\" and s_ != \"died\":\n",
    "            s_ = set_state(s_)  # 继续往下探索\n",
    "\n",
    "        # 开始学习，更新Q-table\n",
    "        # 输入参数为一个state，做出的动作a，收获的奖励r，下一个state s_\n",
    "        RL.learn(str(s), action, reward, str(s_))\n",
    "        q = RL.q_table.loc[str(s), action]\n",
    "\n",
    "        # 将这一轮的s替换为s_,作为下一次的state s\n",
    "        s = s_\n",
    "        # 如果当前游戏状态结束，则跳出循环结束此次迭代，并根据本次结果记录是成功还是失败\n",
    "        if status == True:\n",
    "            right += 1\n",
    "            s = grid.reset()\n",
    "            s = set_state(s)\n",
    "            break\n",
    "\n",
    "# 输出结果\n",
    "print(\"正确次数:\", right)\n",
    "print(\"错误次数:\", 100-right)\n",
    "print(RL.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8335b",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第四部分:作业提交</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ec7d5",
   "metadata": {},
   "source": [
    "一、实验课下课前提交完成代码，如果下课前未完成，请将已经完成的部分进行提交，未完成的部分于之后的实验报告中进行补充  \n",
    "要求:  \n",
    "1)文件格式为：学号-姓名.ipynb  \n",
    "2)【不要】提交文件夹、压缩包、数据集等无关文件，只需提交单个 ipynb 文件即可，如果交错请到讲台前联系助教，删掉之前的错误版本后再进行提交\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c013e19",
   "metadata": {},
   "source": [
    "二、本次实验无需提交实验报告\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801ddb7",
   "metadata": {},
   "source": [
    "三、课堂课件获取地址:https://www.jianguoyun.com/p/DaeV_a8Qp5WhChiIva4FIAA  \n",
    "实验内容获取地址:https://www.jianguoyun.com/p/DTMMhjMQp5WhChjTqK4FIAA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
